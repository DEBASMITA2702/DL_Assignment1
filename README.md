# DA6401 Assignment 1

## Fetching the code files
You need to first clone the github repository containing the files.
```
git clone https://github.com/DEBASMITA2702/DL_Assignment1.git
```
Then change into the code directory.
```
cd DL_Assignment1
```
Ensure that you are in the correct directory before proceeding further.

## Setting up the platform and environment
- ### Local machine
  If you are running the codes in your local machine, execute the following command.
  ```
  pip install -r requirements.txt
  ```
- ### Google colab/Kaggle
  You can directly execute the codes if you are using online platforms like google colab or kaggle.

## Training the model
To train the model, you need to compile and execute the file [train.py](https://github.com/DEBASMITA2702/DL_Assignment1/blob/main/train.py) by using the following command:
```
python train.py
```
The model will run with all the default configurations mentioned in the [SetHyperparamValues.py](https://github.com/DEBASMITA2702/DL_Assignment1/blob/main/SetHyperparamValues.py) file, if we use the above command.\
If you want to pass arguments, then you can customize it by specifying the parameters like ```python train.py <*args>```\
For example,
```
python train.py -wp NewProject -b 32 -e 5 
```
Following arguments are supported : 

|           Name           |   Default Value  | Description                                                               |
| :----------------------: | :-----------:    | :------------------------------------------------------------------------ |
| `-wp`, `--wandb_project` | Debasmita-DA6410-Assignment-1 | Project name used to track experiments in Weights & Biases dashboard |
|  `-we`, `--wandb_entity` |   cs24m015-indian-institute-of-technology-madras    | Wandb Entity used to track experiments in the Weights & Biases dashboard |
|     `-d`, `--dataset`    |  fashion_mnist   | choices:  ["mnist", "fashion_mnist"]                                      |
|     `-e`, `--epochs`     |       10         | Number of epochs to train neural network                                 |
|   `-b`, `--batch_size`   |       128        | Batch size used to train neural network.                                  |
|      `-l`, `--loss`      |  cross_entropy   | choices:  ["mean_squared_error", "cross_entropy"]                         |
|    `-o`, `--optimizer`   |      adam        | choices:  ['stochastic', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']     |
| `-lr`, `--learning_rate` |      5e-3        | Learning rate used to optimize model parameters                           |
|    `-m`, `--momentum`    |      0.999       | Momentum used by momentum and nag optimizers.                             |
|     `-beta`, `--beta`    |      0.999       | Beta used by rmsprop optimizer                                            |
|    `-beta1`, `--beta1`   |      0.9         | Beta1 used by adam and nadam optimizers.                                  |
|    `-beta2`, `--beta2`   |      0.999       | Beta2 used by adam and nadam optimizers.                                  |
|    `-eps`, `--epsilon`   |      1e-5        | Epsilon used by optimizers.                                               |
| `-w_d`, `--weight_decay` |     .0005        | Weight decay used by optimizers.                                          |
|  `-w_i`, `--weight_init` |     he_nor       | choices:  ['random', 'xavier_nor', 'xavier_uni', 'he_nor', 'he_uni']      |
|  `-nhl`, `--num_layers`  |       3          | Number of hidden layers used in feedforward neural network.               |
|  `-sz`, `--hidden_size`  |      128         | Number of hidden neurons in a feedforward layer.                          |
|   `-a`, `--activation`   |     relu         | choices:  ["identity", "sigmoid", "tanh", "relu"]                         |
|   `-c`,`--confusion`     |       0          | Generate confusion matrix. choices:  [0,1]                                |
|    `-t`,`--test`         |       0          | Generate test accuracy. choices:  [0,1]                                   |

All the above arguments can be modified as per requirement through the command line.
  - If prompted to enter the wandb login key, enter the key in the interactive command prompt.

## Testing the model
To test the model, you need to execute the train.py and set the test flag value as 1 in the command line.\
You can specify the particular dataset where you want to test it on.\
For example, if you want the model to run with the default parameters in fashion_mnist dataset, print the test accuracy and generate the confusion matrix, then run the following command: 
```
python train.py -t 1 -c 1 -d fashion_mnist
```
This will make a log of the confusion matrix generated by the model on the dataset into wandb dashboard

## Additional features
  - If you need some clarification on the arguments to be passed, then you can do
    ```
    python train.py --help
    ```

## Wandb Link :
[Report](https://wandb.ai/cs24m015-indian-institute-of-technology-madras/Debasmita-DA6410-Assignment-1/reports/DA6401-Assignment-1-Debasmita-Mandal-CS24M015--VmlldzoxMTYyNTY5NA?accessToken=7cha14k8srm5pzcjfvlgi16su1fx4fyiqdug87mnm9sgpmjr9ie19vyfja3vf3ab)
